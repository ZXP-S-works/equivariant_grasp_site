---
layout: default
---

[Xupeng Zhu](https://zxp-s-works.github.io/), [Dian Wang](https://pointw.github.io), [Ondrej Biza](https://sites.google.com/view/obiza), [Guanang Su](),
[Robin Walters](http://mathserver.neu.edu/robin/), [Robert Platt](http://www.ccs.neu.edu/home/rplatt/)

## Citation

---

```
@article{zhu2022grasp,
  title={Sample Efficient Grasp Learning Using Equivariant Models},
  author={Zhu, Xupeng and Wang, Dian and Biza, Ondrej and Su, Guanang and Walters, Robin and Platt, Robert},
  journal={Proceedings of Robotics: Science and Systems (RSS)},
  year={2022} }
```

## Idea

---
We formulate the planar grasp problem as a contextual bandit problem and use Q learning to learn a policy.

+ Augmented state representation (ASR)

<p align="center">
  <img src="images/ASR.png" alt="ASR" width="400">
</p>

The action space for planar grasp is in 3-dimension (translation along x, y axle, and rotation along z axle). It is
difficult to evaluate the entire action space in one pass for a single neural network. We instead factorize the evaluation
 into translation part and rotational part by q1 and q2 networks.

+ Equivariant neural networks (Equ)

<p align="center">
 <img src="images/equation7.png" alt="q1 equivariance" width="240" style="text-align: left">
 <img src="images/equation8.png" alt="q2 equivariance" width="240" style="text-align: right">
</p>

We recognize that the optimal planar grasp function is SE(2)-equivariant. We use equivariant neural networks to approximate
 the SE(2)-equivariant grasp function.

+ Optimizations for the contextual bandit problem (Opt)

First, we adapt the loss function in ASR so that the target for the q1 network is more accurate. Second, we add an off-policy
learning loss to minimize the gap between q1 and q2 networks. Moreover, we use Boltzmann exploration to explore the action
 space based on the learned q value.


## Baseline Comparison


---

<table>
  <tr>
    <td><img src="images/compare%20with%20all%20baselines%20validation2.png" alt="Simulation results" style="width:100%"></td>
    <td><img src="images/RSS_runs.png" alt="Robot training results" style="width:100%"></td>
  </tr>
  <tr>
    <td>Simulation results, curves are averaged over 4 runs.</td>
     <td>Robot training results, curves are averaged over 4 runs.</td>
  </tr>
 </table>


The baseline comparisons on both simulation and robot show that our method significantly outperforms the baselines. Our
methods advantage shown in these comparisons are two folds: 1) achieves the highest sample efficiency and 2) converges to the highest 
grasp success rate.


## The Robot Training Platform

---

<p align="center">
 <img src="images/UR5_setup.png" alt="ASR" width="175">
 <img src="images/training_set_15.jpg" alt="training set" width="240" style="text-align: left">
 <img src="images/test_set_easy.jpg" alt="testing set" width="235" style="text-align: right">
</p>



All training happens using the 15 objects shown in the middle figure. After training, we evaluate grasp performance on 
the 15 test objects shown in the right figure. Note that the test set is novel with respect to the training set.

At the beginning of training, the 15 training objects are dropped into one of the two trays
by the human operator. Then, we train by attempting to grasp these objects and place them in the other bin. All grasp
attempts are generated by the contextual bandit. When all 15 objects have been transported in this way, training switches
to attempting to grasp from the other bin and transport them into the first. Training continues in this way until 600
grasp attempts have been performed (that is 600 grasp attempts, not 600 successful grasps). A grasp is considered
to be successful if the gripper remains open after closing stops due to a squeezing force. To avoid systematic bias in
the way the robot drops objects into a tray, we sample the drop position randomly from a Gaussian distribution centered
in the middle of the receiving tray.




## The Learned Policy

---

In the simulation, after 1500 grasps training, our equivariant model converges to the grasp success rate of 93.9%.

In the robot, after 600 grasps training, our equivariant model achieves a grasp success rate of 95%.



## Video

---

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/URHr_6-uwuI"
 frameborder="0"
 allow="autoplay;
 encrypted-media"
 allowfullscreen>
</iframe>
</p>

If the video does not display properly, please clik this [video link](https://youtu.be/URHr_6-uwuI).

[View this site in GitHub](https://github.com/ZXP-S-works/equivariant_grasp_site/edit/master/index.md)

